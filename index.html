<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Hide your signer: a Privacy-aware Photorealistic Digital Signer Avatar through Latent Diffusion Model">
  <meta name="keywords" content="Gaussian Avatars, Monocular Avatar Reconstruction, Multi-view Diffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Hide your signer: a Privacy-aware Photorealistic Digital Signer Avatar through Latent Diffusion Model</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Hide your signer: a Privacy-aware Photorealistic Digital Signer Avatar through Latent Diffusion Model</h1>
          <div style="text-align: center; color: #6a0dad; font-size: 1.2em; margin-top: 10px;">
            Submission ID: 1062
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Given a short, monocular video captured by a commodity device such as a smartphone, <b>GAF</b> reconstructs a 3D Gaussian head avatar, which can be re-animated and rendered into photo-realistic novel views. 
        Our key idea is to distill the reconstruction constraints from a multi-view head diffusion model in order to extrapolate to unobserved views and expressions.  
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="container is-max-desktop">
      <div class="column is-full-width">
        <h2 class="title has-text-centered">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            With the democratisation of social media use, the deaf community faces a growing challenge: protecting the privacy of sign language 
            users in shared content. Early sign language synthesis models, which often produce videos that resemble specific individuals, 
            raise concerns about identity disclosure. However, current state-of-the-art solutions often degrade visual quality or fail to 
            properly model important attributes such as mouthing. To this end, we propose a novel approach that leverages a Latent Diffusion 
            Model (LDM) to synthesise photorealistic digital signer avatars from a generated reference image.             We propose a novel sign feature 
            aggregation module that explicitly models the non-manual (<i>e.g.</i>, face) features and the manual (<i>e.g.</i>, hands) features. 
            We show that our proposed module ensures the preservation of the linguistic content while seamlessly utilising the reference image. 
            Extensive experiments on YouTube-SL-25 sign language dataset show that our pipeline achieves superior visual quality compared to 
            state-of-the-art methods, with significant improvements in perceptual metrics such as PSNR and SSIM. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract -->

    <!-- Paper Video -->
    <div class="container is-max-desktop">
      <div class="column is-full-width">
        <h2 class="title has-text-centered">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/QuIYTljvhyg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper Video -->
  </div>
</section>

<section class="section">
  <!-- Method Overview -->
  <div class="container is-max-desktop">
    <div class="column is-full-width">
      <h2 class="title has-text-centered">Method Overview</h2>
      <img src="overview.png" style="width:100%; margin-right:auto; margin-left:auto; margin-top:auto;">
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
      <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
      <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
      <div class="content has-text-justified">
        <p>
          Given a sequence of RGB images from monocular cameras \( \mathcal{I} = \{ \mathbf{I}_i \} \), our objective is to reconstruct dynamic head avatars by optimizing an animatable Gaussian splatting 
          representation \( \mathcal{O} \), which is deformed to each frame as \( \mathcal{O}_i \) by the tracked FLAME mesh \( \mathcal{M}_i \) of \( \mathbf{I}_i \).
          We optimize \( \mathcal{O} \) by minimizing an input view reconstruction loss \( \mathcal{L}_{rec} \), plus a view sampling loss \( \mathcal{L}_{view} \). \( \mathcal{L}_{view} \) compares novel-view renderings of \( \mathcal{O}_i \) from four random viewpoints \( \mathbf{I}_i^{view} \), 
          with pseudo ground truths \( \mathbf{\hat{I}}_i^{view} \), predicted by a multi-view head latent diffusion model. \( \mathbf{\hat{I}}_i^{view} \) are generated by iteratively denoising 4-view latents, 
          guided by the input image \( \mathbf{I}_i \) and normal maps \( \mathbf{N}_i \) rendered from \( \mathcal{M}_i \).  
          A latent upsampler module enhances facial details before decoding the denoised latent into an RGB image.
        </p>
      </div>
    </div>
  </div>
  <!--/ Method Overview -->
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title has-text-centered">Comparisons against Baselines</h2>
    <div style="overflow:hidden;">
      <div class="container">
        <div id="post_images" class="carousel">
          <div class="item-1">
            <video poster="" autoplay="" muted loop="" style="pointer-events: none; width:100%;">
              <source src="./static/videos/DSGS/3002_01.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item-2">
            <video poster="" autoplay="" muted loop="" style="pointer-events: none; width:100%;">
              <source src="./static/videos/DSGS/3013_01.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item-3">
            <video poster="" autoplay="" muted loop="" style="pointer-events: none; width:100%;">
              <source src="./static/videos/DSGS/3054_01.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item-4">
            <video poster="" autoplay="" muted loop="" style="pointer-events: none; width:100%;">
              <source src="./static/videos/DSGS/3073_01.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item-5">
            <video poster="" autoplay="" muted loop="" style="pointer-events: none; width:100%;">
              <source src="./static/videos/DSGS/4332_01.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item-6">
            <video poster="" autoplay="" muted loop="" style="pointer-events: none; width:100%;">
              <source src="./static/videos/DSGS/4348_01.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <div class="content has-text-justified">
        <p>
          Compared to state-of-the-art methods, our approach reconstructs unseen side facial regions in the inputs and consistently produces more favorable and consistent renderings from hold-out views.
        </p>
      </div>
    </div>

    <h2 class="title has-text-centered">Result Gallery</h2>
    <div style="overflow:hidden;">
      <div class="container">
        <div id="post_images" class="carousel">
          <div class="item-1">
            <video poster="" autoplay="" muted loop="" style="pointer-events: none; width:100%;">
              <source src="./static/videos/gallery/gallery1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item-2">
            <video poster="" autoplay="" muted loop="" style="pointer-events: none; width:100%;">
              <source src="./static/videos/gallery/gallery2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item-3">
            <video poster="" autoplay="" muted loop="" style="pointer-events: none; width:100%;">
              <source src="./static/videos/gallery/gallery3.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item-4">
            <video poster="" autoplay="" muted loop="" style="pointer-events: none; width:100%;">
              <source src="./static/videos/gallery/gallery4.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>

    <h2 class="title has-text-centered">Ablation Studies</h2>
    <div style="overflow:hidden;">
      <div class="container">
        <video id="ablation" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/ablation.mp4" type="video/mp4">
        </video>
        <div class="content has-text-justified">
          <p>
            <b>Ablation Studies on different types of diffusion priors.</b> (a) Input; (b) Ground truth; Comparisons between method variants of (c) No diffusion; using (d) Pretrained Stable Diffusion; 
            (e) Personalized Stable Diffusion; (f) Pose-conditioned multi-view diffusion; (g) Our multi-view diffusion using Score Distillation Sampling (SDS) loss; (h) Ours without latent upsampler $\times$2; (i) Ours.  
            Our normal map-conditioned multi-view diffusion priors enable more photo-realistic novel views with identity and appearance consistency, by constraining novel views using pseudo-image ground truths, which are decoded from iteratively denoised latents followed by a latent upsampler.
          </p>
        </div>
      </div>
    </div>

    <h2 class="title has-text-centered">Results of Multi-view Head Diffusion</h2>
    <div style="overflow:hidden;">
      <div class="container">
        <video id="mvgen" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/mvgen.mp4" type="video/mp4">
        </video>
        <div class="content has-text-justified">
          <p>
            Given a single image as input, <b>our Multi-view Head Latent Diffusion</b> can generate identity-preserved, and view-consistent multi-view portrait images. 
          </p>
        </div>
      </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/js/bulma-carousel.min.js"></script>
    <script>
      bulmaCarousel.attach('#post_images', {
        slidesToScroll: 1,
        slidesToShow: 1,
        loop: true,
      });
    </script>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{anonymous1062,
      title={Hide your signer: a Privacy-aware Photorealistic Digital Signer Avatar
through Latent Diffusion Model},
      author={Anonymous},
      booktitle={SIGGRAPH Asia},
      year={2025}
    }</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website source code is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
